delta = deltas, u = sob_red[i,2:5],
hyper = rbind(alphas1, betas1, alphas2, betas2))
}
print(c(n, quantile(red_vec, 1 - typeI), quantile(green_vec, 1 - pwr)))
## save the logits of the probabilities
green_vec_n0 <- green_vec
red_vec_n0 <- red_vec
n0 <- n
## these flags denote whether alpha and beta are too small to explore
## symmetrically around the relevant order statistic
flag1 <- ifelse(2*typeI*m < m0, 1, 0)
flag2 <- ifelse(2*(1 - pwr)*m < m0, 1, 0)
## if beta is too small, we take the smallest m0 order statistics
if (flag2){
low2 <- 1; high2 <- m0
}
## otherwise we explore symmetrically around the order statistic
else{
low2 <- floor((1 - pwr)*m) - 0.5*m0 + 1
high2 <- low2 + m0 - 1
}
## if alpha is too small, we take the largest m0 order statistics
if (flag1){
low1 <- m - m0 + 1; high1 <- m
}
else{
low1 <- floor((1 - typeI)*m) - 0.5*m0 + 1
high1 <- low1 + m0 - 1
}
## we update the indices of the order statistics to reflect the larger
## set of points (m vs. m0)
mid1 <- ceiling((1 - typeI)*m)
mid2 <- floor((1-pwr)*m)
## if we do not satisfy the criterion on the order statistics, we create the linear
## approximation using a larger second sample size
if (sort(red_vec)[mid1] > sort(green_vec)[mid2]){
green_low <- green_vec; red_low <- red_vec
lower <- n
n_low <- n
## choose a larger sample size n(1)
n <- max(ceiling(1.1*n), n + 5)
print(n)
green_vec <- NULL
red_vec <- NULL
## use Algorithm 3 to approximate the posterior probabilities at all m points
for (i in 1:m){
green_vec[i] <- targetPower(n_val = n, q = q, threshold = threshold,
params = as.numeric(green_par[i,]),
delta = deltas, u = sob_green[i,2:5],
hyper = rbind(alphas1, betas1, alphas2, betas2))
red_vec[i] <- targetPower(n_val = n, q = q, threshold = threshold,
params = as.numeric(red_par[i,]),
delta = deltas, u = sob_red[i,2:5],
hyper = rbind(alphas1, betas1, alphas2, betas2))
}
## save the logits of the posterior probabilities
green_vec_n1 <- green_vec
red_vec_n1 <- red_vec
n1 <- n
n_high <- n
green_high <- green_vec; red_high <- red_vec
## obtain the linear approximations for all m points
green_slope <- (green_high - green_low)/(n_high-n_low)
red_slope <- (red_high - red_low)/(n_high-n_low)
green_int <- green_low - green_slope*n_low
red_int <- red_low - red_slope*n_low
## if the criterion on the order statistics is still not satisfied for this
## larger sample size, we need to find a larger upper bound for the binary search
## we do not do this using Algorithm 3 -- just the linear approximations to the
## posterior probabilities on the logit scale.
if (sort(red_vec)[mid1] <= sort(green_vec)[mid2]){
upper <- n
}
else {
n <- ceiling(1.5*n_low)
green_vec <- green_int + green_slope*n
red_vec <- red_int + red_slope*n
upper <- n
while(sort(red_vec)[mid1] > sort(green_vec)[mid2]){
n <- ceiling(1.5*n)
green_vec <- green_int + green_slope*n
red_vec <- red_int + red_slope*n
if (sort(red_vec)[mid1] <= sort(green_vec)[mid2]){
upper <- n
}
print(c(lower, upper, sort(red_vec)[mid1] , sort(green_vec)[mid2]))
}
## if we satisfy the criterion on the order statistics, we create the linear
## approximation using a smaller second sample size
else{
green_high <- green_vec; red_high <- red_vec
upper <- n
n_high <- n
## choose a smaller sample size n(1)
n <- min(floor(0.9*n), n - 5)
print(n)
green_vec <- NULL
red_vec <- NULL
## use Algorithm 3 to approximate the posterior probabilities at all m points
for (i in 1:m){
green_vec[i] <- targetPower(n_val = n, q = q, threshold = threshold,
params = as.numeric(green_par[i,]),
delta = deltas, u = sob_green[i,2:5],
hyper = rbind(alphas1, betas1, alphas2, betas2))
red_vec[i] <- targetPower(n_val = n, q = q, threshold = threshold,
params = as.numeric(red_par[i,]),
delta = deltas, u = sob_red[i,2:5],
hyper = rbind(alphas1, betas1, alphas2, betas2))
}
## save the logits of the posterior probabilities
green_vec_n1 <- green_vec
red_vec_n1 <- red_vec
n1 <- n
n_low <- n
green_low <- green_vec; red_low <- red_vec
## obtain the linear approximations for all m points
green_slope <- (green_high - green_low)/(n_high-n_low)
red_slope <- (red_high - red_low)/(n_high-n_low)
green_int <- green_low - green_slope*n_low
red_int <- red_low - red_slope*n_low
## if the criterion on the order statistics is still satisfied for this
## larger sample size, we need to find a smaller lower bound for the binary search
if (sort(red_vec)[mid1] > sort(green_vec)[mid2]){
lower <- n
}
else{
n <- floor(0.5*n_high)
green_vec <- green_int + green_slope*n
red_vec <- red_int + red_slope*n
lower <- n
while(sort(red_vec)[mid1] <= sort(green_vec)[mid2]){
n <- floor(0.5*n)
green_vec <- green_int + green_slope*n
red_vec <- red_int + red_slope*n
if (sort(red_vec)[mid1] > sort(green_vec)[mid2]){
lower <- n
}
print(c(lower, upper, sort(red_vec)[mid1], sort(green_vec)[mid2]))
}
## implement binary search given these bounds for the sample size
while ((upper - lower) > 1){
n <- ceiling(0.5*(upper + lower))
## use the linear approximations to select the points from the Sobol'
## sequence in a targeted way; these points will be used to explore the
## sampling distributions of posterior probabilities in a targeted way
green_vec <- green_int + green_slope*n
red_vec <- red_int + red_slope*n
## get the indices for these points
sub_green <- which(rank(green_vec) >= low2 & rank(green_vec) <= high2)
sub_red <- which(rank(red_vec) >= low1 & rank(red_vec) <= high1)
green_vec <- NULL
red_vec <- NULL
for (i in 1:length(sub_green)){
green_vec[i] <- targetPower(n_val = n, q = q, threshold = threshold,
params = as.numeric(green_par[sub_green[i],]),
delta = deltas, u = sob_green[sub_green[i],2:5],
hyper = rbind(alphas1, betas1, alphas2, betas2))
red_vec[i] <- targetPower(n_val = n, q = q, threshold = threshold,
params = as.numeric(red_par[sub_red[i],]),
delta = deltas, u = sob_red[sub_red[i],2:5],
hyper = rbind(alphas1, betas1, alphas2, betas2))
}
## for the unselected points, we just assume that their probabilities are less (greater)
## than the relevant order statistic if their anticipated probability based on the
## linear approximations on the logit scale is less (greater) than the relevant order statistic
red_aug <- c(rep(c(min(red_vec)-1), low1 - 1), red_vec, rep(c(max(red_vec)+1), m - high1))
green_aug <- c(rep(c(min(green_vec)-1), low2 - 1), green_vec, rep(c(max(green_vec)+1), m - high2))
if (sort(red_aug)[mid1] <= sort(green_aug)[mid2]){
green_max <- green_vec; red_max <- red_vec
sub_green_max <- sub_green; sub_red_max <- sub_red
upper <- n
}
else{
lower <- n
}
print(c(lower, upper, sort(red_aug)[mid1], sort(green_aug)[mid2]))
}
n <- upper
print(n)
## we don't need to reapproximate the same posterior probabilities if we have already explored
## this sample size
if (n == n_high){
## do not need to do this; it is just so we have three sample sizes to construct each contour plot
n <- ifelse(n0 == n_high, max(ceiling(1.1*n0), n0 + 5), max(ceiling(1.1*n1), n1 + 5))
print(n)
green_vec <- NULL
red_vec <- NULL
## use Algorithm 3 to approximate the posterior probabilities at all m points
for (i in 1:m){
green_vec[i] <- targetPower(n_val = n, q = q, threshold = threshold,
params = as.numeric(green_par[i,]),
delta = deltas, u = sob_green[i,2:5],
hyper = rbind(alphas1, betas1, alphas2, betas2))
red_vec[i] <- targetPower(n_val = n, q = q, threshold = threshold,
params = as.numeric(red_par[i,]),
delta = deltas, u = sob_red[i,2:5],
hyper = rbind(alphas1, betas1, alphas2, betas2))
}
## save the logits of the posterior probabilities
green_vec_n2 <- green_vec
red_vec_n2 <- red_vec
n2 <- n
n_final <- ifelse(n1 > n0, 1, 0)
## first component of list is (n, gamma, relevant order statistic of green probabilities, confirmatory type I error
## estimate, confirmatory power estimate, n^((0)), n^((1)), n^((2)))
## components 2 to 6 of the list are only needed for the contour plot (they are the posterior probabilities from
## the red and green regions for the sample sizes n^((0)), n^((1)), and n^((2)))
## we sort the sample sizes and the posterior probabilities to help with making the plot
results <- list(c(get(paste0("n", n_final)), 1/(1+exp(-as.numeric(sort(get(paste0("red_vec_n", n_final)))[mid1]))),
1/(1+exp(-as.numeric(sort(get(paste0("green_vec_n", n_final)))[mid2]))),
as.numeric(mean(get(paste0("red_vec_n", n_final)) > sort(get(paste0("red_vec_n", n_final)))[mid1])),
as.numeric(mean(get(paste0("green_vec_n", n_final)) > sort(get(paste0("red_vec_n", n_final)))[mid1])),
sort(c(n0, n1, n2))),
get(paste0("red_vec_n", which.min(c(n0, n1, n2))-1)), get(paste0("green_vec_n", which.min(c(n0, n1, n2))-1)),
get(paste0("red_vec_n", which(sort(c(n0, n1, n2))[2] == c(n0, n1, n2))-1)), get(paste0("green_vec_n", which(sort(c(n0, n1, n2))[2] == c(n0, n1, n2))-1)),
get(paste0("red_vec_n", which.max(c(n0, n1, n2))-1)), get(paste0("green_vec_n", which.max(c(n0, n1, n2))-1)))
if (contour == TRUE){
return(results)
}
else{
return(results[[1]][1:5])
}
if (n == n_low){
## do not need to do this; it is just so we have three sample sizes to construct each contour plot
n <- ifelse(n0 == n_low, min(floor(0.9*n0), n0 - 5), min(floor(0.9*n1), n1 - 5))
print(n)
green_vec <- NULL
red_vec <- NULL
## use Algorithm 3 to approximate the posterior probabilities at all m points
for (i in 1:m){
green_vec[i] <- targetPower(n_val = n, q = q, threshold = threshold,
params = as.numeric(green_par[i,]),
delta = deltas, u = sob_green[i,2:5],
hyper = rbind(alphas1, betas1, alphas2, betas2))
red_vec[i] <- targetPower(n_val = n, q = q, threshold = threshold,
params = as.numeric(red_par[i,]),
delta = deltas, u = sob_red[i,2:5],
hyper = rbind(alphas1, betas1, alphas2, betas2))
}
## save the logits of the posterior probabilities
green_vec_n2 <- green_vec
red_vec_n2 <- red_vec
n2 <- n
n_final <- ifelse(n1 < n0, 1, 0)
print(c("End:", n0, n1, n2))
## first component of list is (n, gamma, relevant order statistic of green probabilities, confirmatory type I error
## estimate, confirmatory power estimate, n^((0)), n^((1)), n^((2)))
## components 2 to 6 of the list are only needed for the contour plot (they are the posterior probabilities from
## the red and green regions for the sample sizes n^((0)), n^((1)), and n^((2)))
## we sort the sample sizes and the posterior probabilities to help with making the plot
results <- list(c(get(paste0("n", n_final)), 1/(1+exp(-as.numeric(sort(get(paste0("red_vec_n", n_final)))[mid1]))),
1/(1+exp(-as.numeric(sort(get(paste0("green_vec_n", n_final)))[mid2]))),
as.numeric(mean(get(paste0("red_vec_n", n_final)) > sort(get(paste0("red_vec_n", n_final)))[mid1])),
as.numeric(mean(get(paste0("green_vec_n", n_final)) > sort(get(paste0("red_vec_n", n_final)))[mid1])),
sort(c(n0, n1, n2))),
get(paste0("red_vec_n", which.min(c(n0, n1, n2))-1)), get(paste0("green_vec_n", which.min(c(n0, n1, n2))-1)),
get(paste0("red_vec_n", which(sort(c(n0, n1, n2))[2] == c(n0, n1, n2))-1)), get(paste0("green_vec_n", which(sort(c(n0, n1, n2))[2] == c(n0, n1, n2))-1)),
get(paste0("red_vec_n", which.max(c(n0, n1, n2))-1)), get(paste0("green_vec_n", which.max(c(n0, n1, n2))-1)))
if (contour == TRUE){
return(results)
}
else{
return(results[[1]][1:5])
}
## otherwise, we approximate the remaining posterior probabilities that were not
## selected at the final sample size recommendation
green_vec <- NULL
red_vec <- NULL
remain_green <- subset(1:m, ! 1:m %in% sub_green_max)
remain_red <- subset(1:m, ! 1:m %in% sub_red_max)
for (i in remain_green){
green_vec[i] <- targetPower(n_val = n, q = q, threshold = threshold,
params = as.numeric(green_par[i,]),
delta = deltas, u = sob_green[i,2:5],
hyper = rbind(alphas1, betas1, alphas2, betas2))
}
for (i in remain_red){
red_vec[i] <- targetPower(n_val = n, q = q, threshold = threshold,
params = as.numeric(red_par[i,]),
delta = deltas, u = sob_red[i,2:5],
hyper = rbind(alphas1, betas1, alphas2, betas2))
}
green_vec[sub_green_max] <- green_max; red_vec[sub_red_max] <- red_max
## save the logits of the posterior probabilities
green_vec_n2 <- green_vec
red_vec_n2 <- red_vec
n2 <- n
print(c(n, sort(red_vec)[mid1], sort(green_vec)[mid2]))
## first component of list is (n, gamma, relevant order statistic of green probabilities, confirmatory type I error
## estimate, confirmatory power estimate, n^((0)), n^((1)), n^((2)))
## components 2 to 6 of the list are only needed for the contour plot (they are the posterior probabilities from
## the red and green regions for the sample sizes n^((0)), n^((1)), and n^((2)))
## we sort the sample sizes and the posterior probabilities to help with making the plot
results <- list(c(n, 1/(1 + exp(-as.numeric(sort(red_vec)[mid1]))),
1/(1 + exp(-as.numeric(sort(green_vec)[mid2]))),
as.numeric(mean(red_vec > sort(red_vec)[mid1])), as.numeric(mean(green_vec > sort(red_vec)[mid1])),
sort(c(n0, n1, n2))),
get(paste0("red_vec_n", which.min(c(n0, n1, n2))-1)), get(paste0("green_vec_n", which.min(c(n0, n1, n2))-1)),
get(paste0("red_vec_n", which(sort(c(n0, n1, n2))[2] == c(n0, n1, n2))-1)), get(paste0("green_vec_n", which(sort(c(n0, n1, n2))[2] == c(n0, n1, n2))-1)),
get(paste0("red_vec_n", which.max(c(n0, n1, n2))-1)), get(paste0("green_vec_n", which.max(c(n0, n1, n2))-1)))
if (contour == TRUE){
return(results)
}
else{
return(results[[1]][1:5])
}
## alternative function to explore sample sizes using all points from the
## hypercube for each sample size that we consider (slightly modified for use
## with the Weibull example)
findGammaFull <- function(green_par, red_par, pwr, typeI, deltas, q,
alphas1, betas1, alphas2, betas2, m = 8192, m0 = 512, threshold = 0.9,
seed_green = 1, seed_red = 2, upper_init = 1000){
## generate Sobol' sequence for each sampling distribution
sob_green <- sobol(m, d = 5, randomize = "digital.shift", seed = seed_green)
sob_red <- sobol(m, d = 5, randomize = "digital.shift", seed = seed_red)
## index the eta values from the design priors such that the anticipated value for
## theta increases with the index; we do this using the first dimension of the Sobol' sequence
green_means1 <- log(green_par[,1]*log(1/(1 - threshold))^(1/green_par[,2]))
green_means2 <- log(green_par[,3]*log(1/(1 - threshold))^(1/green_par[,4]))
green_diff <- green_means1 - green_means2
green_par <- green_par[order(green_diff),]
green_par <- green_par[ceiling(length(green_diff)*sob_green[,1]),]
## repeat this process for the red region
red_means1 <- log(red_par[,1]*log(1/(1 - threshold))^(1/red_par[,2]))
red_means2 <- log(red_par[,3]*log(1/(1 - threshold))^(1/red_par[,4]))
red_diff <- red_means1 - red_means2
red_par <- red_par[order(red_diff),]
red_par <- red_par[ceiling(length(red_diff)*sob_red[,1]),]
if (m < m0){stop("m0 should be less than m")}
lower <- 1
n <- upper_init
upper <- upper_init
green_vec <- rep(0, m)
red_vec <- rep(1, m)
mid1 <- ceiling((1 - typeI)*m)
mid2 <- floor((1-pwr)*m)
## find a larger upper bound if the initial upper bound provided
## is not large enough
while (sort(red_vec)[mid1] > sort(green_vec)[mid2]){
green_vec <- NULL
red_vec <- NULL
## implement Algorithm 3 for each of the first m0 points
for (i in 1:m){
green_vec[i] <- targetPower(n_val = n, q = q, threshold = threshold,
params = as.numeric(green_par[i,]),
delta = deltas, u = sob_green[i,2:5],
hyper = rbind(alphas1, betas1, alphas2, betas2))
red_vec[i] <- targetPower(n_val = n, q = q, threshold = threshold,
params = as.numeric(red_par[i,]),
delta = deltas, u = sob_red[i,2:5],
hyper = rbind(alphas1, betas1, alphas2, betas2))
}
## increase upper bound if the criterion on the order statistics is not satisfied
if (sort(red_vec)[mid1] <= sort(green_vec)[mid2]){
green_max <- green_vec; red_max <- red_vec
upper <- n
n_max <- n
}
else{
lower <- n
green_min <- green_vec; red_min <- red_vec
n_min <- n
n <- 2*n
}
print(c(lower, upper, sort(red_vec)[mid1], sort(green_vec)[mid2]))
}
## implement binary search given these bounds for the sample size
while ((upper - lower) > 1){
n <- ceiling(0.5*(upper + lower))
green_vec <- NULL
red_vec <- NULL
for (i in 1:m){
green_vec[i] <- targetPower(n_val = n, q = q, threshold = threshold,
params = as.numeric(green_par[i,]),
delta = deltas, u = sob_green[i,2:5],
hyper = rbind(alphas1, betas1, alphas2, betas2))
red_vec[i] <- targetPower(n_val = n, q = q, threshold = threshold,
params = as.numeric(red_par[i,]),
delta = deltas, u = sob_red[i,2:5],
hyper = rbind(alphas1, betas1, alphas2, betas2))
}
if (sort(red_vec)[mid1] <= sort(green_vec)[mid2]){
green_max <- green_vec; red_max <- red_vec
upper <- n
}
else{
lower <- n
}
print(c(lower, upper, sort(red_vec)[mid1], sort(green_vec)[mid2]))
}
n <- upper
print(n)
## list is (n, gamma, relevant order statistic of green probabilities, confirmatory type I error
## estimate, confirmatory power estimate)
return(list(c(n, 1/(1 + exp(-as.numeric(sort(red_vec)[mid1]))),
1/(1 + exp(-as.numeric(sort(green_vec)[mid2]))),
as.numeric(mean(red_vec > sort(red_vec)[mid1])), as.numeric(mean(green_vec > sort(red_vec)[mid1])))))
}
temp_res <-  findGamma(green_par = greens, red_par = reds,
pwr = 0.7, typeI = 0.1, deltas = c(log(1/1.2), log(1.2)), q = 1,
alphas1, betas1, alphas2, betas2, m = 8192,
seed_green = i + 8000, seed_red = i + 9000, contour = TRUE)
tic <- Sys.time()
temp_res <-  findGamma(green_par = greens, red_par = reds,
pwr = 0.7, typeI = 0.1, deltas = c(log(1/1.2), log(1.2)), q = 1,
alphas1, betas1, alphas2, betas2, m = 8192,
seed_green = i + 8000, seed_red = i + 9000, contour = FALSE)
toc <- Sys.time()
toc - tic
tic <- Sys.time()
temp_res <- findGammaFull(green_par = greens, red_par = reds,
pwr = 0.7, typeI = 0.1, deltas = c(log(1/1.2), log(1.2)), q = 1,
alphas1, betas1, alphas2, betas2, m = 8192,
seed_green = i + 8000, seed_red = i + 9000)
toc <- Sys.time()
toc - tic
0.56*60
theta_pre <- rbeta(100000, 20, 80)
plot(density(theta_pre))
plot(density(theta_pre), xlim = c(0,0.5), ylim = c(0, 20))
theta_post <- rbeta(100000, 20 + 25, 80 + 75)
lines(density(theta_post), col = "red")
44/(200 - 2)
fn_opt <- function(x, alpha, beta, mle, n){
return(n*(mle/x - (1-mle)/(1-x))*(x - mle) - 0.5*n*(x-mle)^2*(mle/x^2 + (1-mle)/(1 - x)^2 + (alpha-1)*log(x) + (beta-1)*log(1-x)))
}
?optim
optim(0.25, fn_opt, alpha = 20, beta = 80, mle = 0.25, n = 100, method = "Nelder-Mead")
fn_opt <- function(x, alpha, beta, mle, n){
return(n*(mle/x - (1-mle)/(1-x))*(x - mle) - 0.5*n*(x-mle)^2*(mle/x^2 + (1-mle)/(1 - x))^2 + (alpha-1)*log(x) + (beta-1)*log(1-x))
}
optim(0.25, fn_opt, alpha = 20, beta = 80, mle = 0.25, n = 100, method = "Nelder-Mead")
fn_opt <- function(x, alpha, beta, mle, n){
return(-1*(n*(mle/x - (1-mle)/(1-x))*(x - mle) - 0.5*n*(x-mle)^2*(mle/x^2 + (1-mle)/(1 - x))^2 + (alpha-1)*log(x) + (beta-1)*log(1-x)))
}
optim(0.25, fn_opt, alpha = 20, beta = 80, mle = 0.25, n = 100, method = "Nelder-Mead")
44/198
abline(v = 0.2222)
fn_opt <- function(x, alpha, beta, mle, n){
return(-1*(n*(mle/x - (1-mle)/(1-x))*(x - mle) - 0.5*n*(x-mle)^2*(mle/x^2 + (1-mle)/(1 - x)^2) + (alpha-1)*log(x) + (beta-1)*log(1-x)))
}
optim(0.25, fn_opt, alpha = 20, beta = 80, mle = 0.25, n = 100, method = "Nelder-Mead")
fn_opt <- function(x, alpha, beta, mle, n){
return(-( 0.5*n*(x-mle)^2*(1/mle +1/(1 - mle) + (alpha-1)*log(x) + (beta-1)*log(1-x)))
}
fn_opt <- function(x, alpha, beta, mle, n){
return(-( 0.5*n*(x-mle)^2*(1/mle +1/(1 - mle)) + (alpha-1)*log(x) + (beta-1)*log(1-x)))
}
optim(0.25, fn_opt, alpha = 20, beta = 80, mle = 0.25, n = 100, method = "Nelder-Mead")
fn_opt <- function(x, alpha, beta, mle, n){
return(-( -0.5*n*(x-mle)^2*(1/mle +1/(1 - mle)) + (alpha-1)*log(x) + (beta-1)*log(1-x)))
}
optim(0.25, fn_opt, alpha = 20, beta = 80, mle = 0.25, n = 100, method = "Nelder-Mead")
fn_opt2 <- function(x, alpha, beta, mle, ach, n){
return(-1*(n*(mle/ach - (1-mle)/(1-ach))*(x - ach) - 0.5*n*(x-ach)^2*(mle/ach^2 + (1-mle)/(1 - ach)^2) + (alpha-1)*log(x) + (beta-1)*log(1-x)))
}
optim(0.25, fn_opt2, alpha = 20, beta = 80, mle = 0.25, n = 100, method = "Nelder-Mead")
optim(0.25, fn_opt2, alpha = 20, beta = 80, mle = 0.25, n = 100, ach = 0.2210449, method = "Nelder-Mead")
optim(0.25, fn_opt2, alpha = 20, beta = 80, mle = 0.25, n = 100, ach = 0.2222656, method = "Nelder-Mead")
optim(0.25, fn_opt2, alpha = 20, beta = 80, mle = 0.25, n = 100, ach = 0.25, method = "Nelder-Mead")
plot(density(theta_pre), xlim = c(0,0.5), ylim = c(0, 20))
theta_post <- rbeta(100000, 20 + 35, 80 + 65)
lines(density(theta_post), col = "red")
fn_opt <- function(x, alpha, beta, mle, n){
return(-( -0.5*n*(x-mle)^2*(1/mle +1/(1 - mle)) + (alpha-1)*log(x) + (beta-1)*log(1-x)))
}
optim(0.25, fn_opt, alpha = 20, beta = 80, mle = 0.35, n = 100, method = "Nelder-Mead")
54/198
fn_opt2 <- function(x, alpha, beta, mle, ach, n){
return(-1*(n*(mle/ach - (1-mle)/(1-ach))*(x - ach) - 0.5*n*(x-ach)^2*(mle/ach^2 + (1-mle)/(1 - ach)^2) + (alpha-1)*log(x) + (beta-1)*log(1-x)))
}
optim(0.25, fn_opt2, alpha = 20, beta = 80, mle = 0.25, n = 100, ach = 0.35, method = "Nelder-Mead")
optim(0.25, fn_opt2, alpha = 20, beta = 80, mle = 0.35, n = 100, ach = 0.35, method = "Nelder-Mead")
optim(0.25, fn_opt2, alpha = 20, beta = 80, mle = 0.35, n = 100, ach = 0.2667969, method = "Nelder-Mead")
54/198
theta_pre <- rbeta(100000, 20, 80)
plot(density(theta_pre), xlim = c(0,0.5), ylim = c(0, 20))
## MLE is 35%
theta_post <- rbeta(100000, 20 + 35, 80 + 65)
lines(density(theta_post), col = "red")
theta_pre <- rbeta(1000000, 20, 80)
plot(density(theta_pre), xlim = c(0,0.5), ylim = c(0, 20))
## MLE is 35%
theta_post <- rbeta(1000000, 20 + 35, 80 + 65)
lines(density(theta_post), col = "red")
fn_opt <- function(x, alpha, beta, mle, ach, n){
return(-1*(n*(mle/ach - (1-mle)/(1-ach))*(x - ach) - 0.5*n*(x-ach)^2*(mle/ach^2 + (1-mle)/(1 - ach)^2) + (alpha-1)*log(x) + (beta-1)*log(1-x)))
}
optim(0.25, fn_opt, alpha = 20, beta = 80, mle = 0.35, n = 100, ach = 0.35, method = "Nelder-Mead")
lines(density(rnorm(1000000, 0.2667969, sqrt(0.2667969*(1-0.2667969)/(200)))), col = "blue")
iter1 <- optim(0.25, fn_opt, alpha = 20, beta = 80, mle = 0.35, n = 100, ach = 0.35, method = "Nelder-Mead")$par
lines(density(rnorm(1000000, iter1, sqrt(iter1*(1-iter1)/(200)))), col = "blue")
plot(density(theta_pre), xlim = c(0,0.5), ylim = c(0, 20))
## MLE is 35%
theta_post <- rbeta(1000000, 20 + 35, 80 + 65)
lines(density(theta_post), col = "red")
fn_opt <- function(x, alpha, beta, mle, ach, n){
return(-1*(n*(mle/ach - (1-mle)/(1-ach))*(x - ach) - 0.5*n*(x-ach)^2*(mle/ach^2 + (1-mle)/(1 - ach)^2) + (alpha-1)*log(x) + (beta-1)*log(1-x)))
}
iter1 <- optim(0.25, fn_opt, alpha = 20, beta = 80, mle = 0.35, n = 100, ach = 0.35, method = "Nelder-Mead")$par
lines(density(rnorm(1000000, iter1, sqrt(iter1*(1-iter1)/(200)))), col = "blue")
iter2 <- optim(0.25, fn_opt, alpha = 20, beta = 80, mle = 0.35, n = 100, ach = iter1, method = "Nelder-Mead")$par
lines(density(rnorm(1000000, iter2, sqrt(iter2*(1-iter2)/(200)))), col = "forestgreen")
iter2
iter3 <- optim(0.25, fn_opt, alpha = 20, beta = 80, mle = 0.35, n = 100, ach = iter2, method = "Nelder-Mead")$par
iter2
iter3
54/200
54/198
iter3 <- optim(0.25, fn_opt, alpha = 20, beta = 80, mle = 0.35, n = 100, ach = iter2, method = "Nelder-Mead")$par
lines(density(rnorm(1000000, iter3, sqrt(iter3*(1-iter3)/(200)))), col = "purple")
8192*5
